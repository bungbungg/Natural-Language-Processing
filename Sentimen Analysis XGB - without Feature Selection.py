# -*- coding: utf-8 -*-
"""Halodoc SMOTEENN + XGB + VADER (Metode Lexicon untuk pelabelan) + Tanpa Feature Selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QOokBqlvQm5_UEyWsDlkkM-2JxFLGBLI

## Klasifikasi Text
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Untuk mempermudah, simpan setiap objek agar dapat digunakan untuk pemodelan maupun deployment. Gunakan library Pickle
import pickle

from nltk.probability import FreqDist


# %matplotlib inline

!pip -q install sastrawi

import nltk
nltk.download('stopwords')

"""# Data Acquisition & Data Preprocessing"""

data = pd.read_excel('/content/data_label.xlsx')
data.head()



data.drop(['created_at','id_str','quote_count','reply_count','retweet_count','favorite_count','lang', 'user_id_str', 'conversation_id_str','tweet_url','username'],
        axis=1,
        inplace=True)

data

## CASE FOLDING
import re

# Buat fungsi untuk langkah case folding
def casefolding(text):
    text = text.lower()                               # Mengubah teks menjadi lower case
    text = re.sub('@\w+','',text)                     # Menghapus mention username
    text = re.sub('#\w+','',text)                     # Menghapus hashtag
    text = re.sub(r'https?://\S+|www\.\S+', '', text) # Menghapus URL
    text = re.sub(r'[-+]?[0-9]+', '', text)           # Menghapus angka
    text = re.sub(r'[^\w\s]','', text)                # Menghapus karakter tanda baca
    return text

## WORD NORMALIZATION
key_norm = pd.read_csv('https://raw.githubusercontent.com/Ismat-p/Text-Mining/main/key_norm.csv')

def text_normalize(text):
    text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])
    text = str.lower(text)
    return text

## FILTERING/STOPWORDS REMOVAL
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

stopwords_ind = stopwords.words('indonesian')
# Buat fungsi untuk langkah stopword removal
tambahan = ['halodoc','nih', 'ya', 'loh', 'sih', 'amp', 'si', 'banget', 'nya', 'ku', 'nghahah']

stopwords_ind += tambahan

def remove_stop_words(text):
    clean_words = []
    text = text.split()
    for word in text:
        if word not in stopwords_ind:
            clean_words.append(word)
    return " ".join(clean_words)

## STEMMING
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Buat fungsi untuk langkah stemming bahasa Indonesia
def stemming(text):
    text = stemmer.stem(text)
    return text

"""## Text Preprocessing Pipeline"""

# Buat fungsi untuk menggabungkan seluruh langkah text preprocessing
def text_preprocessing_process(text):
    text = casefolding(text)
    text = text_normalize(text)
    text = remove_stop_words(text)
    text = stemming(text)
    return text

data['clean_teks'] = data['full_text'].apply(text_preprocessing_process)
data.to_csv('clean_data',sep=';', index=False)

data_clean = pd.read_csv('/content/clean_data', sep=';')
data_clean

def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens
    text = word_tokenize(text)
    return text

import nltk
nltk.download('punkt')

data_clean['text_preprocessed'] = data_clean['clean_teks'].apply(tokenizingText)
data_clean

data_clean.to_csv('token_data.csv',sep=';', index=False)

pip install googletrans==3.1.0a0

from googletrans import Translator
translator = Translator()
translation = {}
for row in data_clean['full_text']:
    translation[row] = translator.translate(row).text
translation

data_clean['full_text'].replace(translation, inplace=True)
data_clean['full_text'].head(10)

data_clean.to_csv('translate_halodoc.csv', encoding='utf8', index=False, sep=';')

data_translate = pd.read_csv('translate_halodoc.csv', sep=';')
data_translate



!pip install vaderSentiment

nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()

data_translate['skor'] = data_translate['full_text'].apply(lambda txt: sid.polarity_scores(txt))

data_translate['negatif'] = data_translate['skor'].apply(lambda txt: txt['neg'])
data_translate['netral'] = data_translate['skor'].apply(lambda txt: txt['neu'])
data_translate['positif'] =data_translate['skor'].apply(lambda txt: txt['pos'])
data_translate['compound'] = data_translate['skor'].apply(lambda txt: txt['compound'])

def polarity_score(compound):
    if compound > 0.05:
        return "positif"
    elif compound < -0.5:
        return "negatif"
    elif compound >= -0.05 and compound < 0.05:
        return "netral"

data_translate['sentiment'] = data_translate['compound'].apply(lambda val: polarity_score(val))

data_translate.head()

data_translate.loc[data_translate['compound'] < 0, 'Sentimen'] = 'Negatif'
data_translate.loc[data_translate['compound'] == 0, 'Sentimen'] = 'Netral'
data_translate.loc[data_translate['compound'] > 0, 'Sentimen'] = 'Positif'
data_translate.head(15)

data_translate.Sentimen.value_counts()



# melihat 10 data sentimen negatif terakhir
data_translate.loc[data_translate['Sentimen']== 'Negatif'][-10:]

from textblob import TextBlob
import pandas as pd

def get_tweet_sentiment(x):
    '''Utility function to classify sentiment of passed tweet using textblob's sentiment method'''
    # create TextBlob object of passed tweet text
    analysis = TextBlob(x)

    # set sentiment
    if analysis.sentiment.polarity > 0:
        return 'Positif'
    else:
        return 'Negatif'


data_translate['sentimen_bloob'] = data_translate.full_text.apply(get_tweet_sentiment)
data_translate.head(5)

"""# Visualisasi Data"""

height = data_translate['sentimen_bloob'].value_counts()
labels = height.keys()
y_pos = np.arange(len(labels))

plt.figure(figsize=(7,4), dpi=80)
plt.ylim(0,1000)
plt.title('Distribusi Kategori Tweet', fontweight='bold')
plt.xlabel('Kategori', fontweight='bold')
plt.ylabel('Jumlah', fontweight='bold')
plt.bar(y_pos, height, color=['deepskyblue','red'])
plt.xticks(y_pos, labels)
plt.show()

plt.pie(data_translate['Sentimen'].value_counts(),labels=labels,autopct='%1.1f%%', colors = ['deepskyblue','red'])
plt.show()

"""# Feature Engineering"""

data_ready = pd.read_csv('/content/translate_halodoc.csv', sep=';')
data_ready

data_ready['label'] = data_translate['Sentimen']
data_ready['label_bloob'] = data_translate['sentimen_bloob']

data_ready

# Pisahkan kolom feature dan target
X = data_ready['clean_teks']
y = data_ready['label_bloob']

"""# Feature Extraction"""

#Mengonversi kumpulan dokumen mentah menjadi matriks fitur TF-IDF
#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html


from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf = TfidfVectorizer(ngram_range=(1,1))
tf_idf.fit(X)

X_tf_idf = tf_idf.transform(X)

# Melihat Jumlah Fitur
print(len(tf_idf.get_feature_names_out()))

# Melihat fitur-fitur apa saja yang ada di dalam corpus
print(tf_idf.get_feature_names_out())

# Melihat matriks jumlah token menggunakan TF IDF, lihat perbedaannya dengan metode BoW
# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)

X_tf_idf = tf_idf.transform(X).toarray()

X_tf_idf

data_tf_idf = pd.DataFrame(X_tf_idf, columns=tf_idf.get_feature_names_out())
data_tf_idf



"""# Feature Selection"""

# Mengubah nilai data tabular tf-idf menjadi array agar dapat dijalankan pada proses seleksi fitur
X = np.array(data_tf_idf)
y = np.array(y)

#Pilih fitur sesuai dengan k skor tertinggi.
#https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html

#Hitung statistik chi-squared antara setiap fitur dan kelas non-negatif.
#https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# 10 fitur dengan statistik chi-kuadrat tertinggi dipilih
chi2_features = SelectKBest(chi2, k=X.shape[1])
X_kbest_features = chi2_features.fit_transform(X, y)

print('Original feature number:', X.shape[1])

# chi2_features.scores_ adalah nilai chi-square, semakin tinggi nilainya maka semakin baik fiturnya
data_chi2 = pd.DataFrame(chi2_features.scores_, columns=['nilai'])
data_chi2

# Menampilkan fitur beserta nilainya
feature = tf_idf.get_feature_names_out()
data_chi2['fitur'] = feature
data_chi2

# Mengurutkan fitur terbaik
data_chi2.sort_values(by='nilai', ascending=False)

# Menampilkan mask pada feature yang diseleksi
# False berarti fitur tidak terpilih dan True berarti fitur terpilih
mask = chi2_features.get_support()
mask



# Menampilkan fitur-fitur terpilih berdasarkan mask atau nilai tertinggi yang sudah dikalkulasi pada Chi-Square
new_feature = []
for bool, f in zip(mask, feature):
    if bool:
        new_feature.append(f)
    selected_feature = new_feature


# Membuat vocabulary baru berdasarkan fitur yang terseleksi
#Ini digunakan untuk mengenerate fitur vector tf-idf pada proses deployment

new_selected_features = {}

for (k,v) in tf_idf.vocabulary_.items():
    if k in selected_feature:
        new_selected_features[k] = v

new_selected_features

# Menampilkan fitur-fitur yang sudah diseleksi
# Beserta nilai vektornya pada keseluruhan data untuk dijalankan pada proses machine learning

# Hanya k fitur yang terpilih sesuai parameter k yang ditentukan sebelumnya

data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)
data_selected_feature

#Menyimpan vektor dari vocabulary di atas dalam bentuk pickle (.pkl)
pickle.dump(new_selected_features,open("selected_feature_tf-idf2.pkl","wb"))

selected_x = X_kbest_features
selected_x

"""## Pembagian data training, testing, dan oversampling dengan SMOTEENN"""

#Import Library
import random
from sklearn.model_selection import train_test_split

X = selected_x
y = data_ready.label_bloob

pip install imblearn --user



from imblearn.combine import SMOTEENN
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import preprocessing

over_sample = SMOTEENN(sampling_strategy='auto')
le = preprocessing.LabelEncoder()
y = le.fit_transform(y)

print(f"Initial set observations {X.shape[0]}")
print(f"Initial set target classes {len(set(y))}")

X, y = over_sample.fit_resample(X, y)                       # jika error berarti rasio antar kelas sudah seimbang


print(f"Modified set observations {X.shape[0]}")
print(f"Modified set target classes {len(set(y))}")

X_smoteenn= np.array(X)       # jadikan X.todense() jika error
X_smoteenn

y_smoteenn = np.array(y)
y_smoteenn

"""# Modelling"""



from xgboost import XGBClassifier
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer
import seaborn as sns

def training_xgb(X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb, preproc):

    clf = XGBClassifier()
    clf.fit(X_train_xgb, y_train_xgb)

    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])

    y_pred = clf.predict(X_test_xgb)
    cf_matrix = confusion_matrix(y_test_xgb, y_pred)
    print(cf_matrix)

    sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g')
    f1 = f1_score(y_pred, y_test_xgb, average = 'weighted')
    pres = precision_score(y_pred, y_test_xgb, average = 'weighted')
    rec = recall_score(y_pred, y_test_xgb, average = 'weighted')
    acc = accuracy_score(y_pred, y_test_xgb)




    res = res.append({'Preprocessing': preproc, 'Model': 'XGB', 'Precision': pres,
                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)

    return res

#Memisihkan data training dan data testing dengan perbandingan 80:20
X_train, X_test, y_train, y_test = train_test_split(X_smoteenn,y_smoteenn,test_size=0.2, random_state=57)

#Algoritme

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import KNeighborsClassifier
import seaborn as sns

def training_knn(X_train, X_test, y_train, y_test, preproc):

    clf = KNeighborsClassifier(n_neighbors=7, metric='cosine')
    clf.fit(X_train, y_train)

    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])

    y_pred = clf.predict(X_test)
    cf_matrix = confusion_matrix(y_test, y_pred)
    print(cf_matrix)

    sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g')
    f1 = f1_score(y_pred, y_test, average = 'weighted')
    pres = precision_score(y_pred, y_test, average = 'weighted')
    rec = recall_score(y_pred, y_test, average = 'weighted')
    acc = accuracy_score(y_pred, y_test)




    res = res.append({'Preprocessing': preproc, 'Model': 'K-NN', 'Precision': pres,
                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)

    return res

clf = KNeighborsClassifier(n_neighbors=4, metric='cosine')
model = clf.fit(X_train, y_train)

# menyimpan model
from joblib import dump
dump(model, filename="model_knn_save.joblib")

from joblib import load
from sklearn.feature_extraction.text import TfidfVectorizer

pipeline = load("/content/model_knn_save.joblib")

data = input(" Masukan kata:\n")
data = text_preprocessing_process(data)

#load
tfidf = TfidfVectorizer

loaded_vec = TfidfVectorizer(decode_error="replace", vocabulary=set(pickle.load(open("/content/selected_feature_tf-idf2.pkl", "rb"))))
hasil = pipeline.predict(loaded_vec.fit_transform([data]))
print("Hasil Prediksi:\n", hasil)

import cv2
originalImage = cv2.imread('cloud.jpg')
grayImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2GRAY)
(thresh, cloud_mask) = cv2.threshold(grayImage, 100, 255, cv2.THRESH_BINARY)



X_trainsmoteenn, X_testsmoteenn, y_trainsmoteenn, y_testsmoteenn = train_test_split(X_smoteenn,y_smoteenn,test_size=0.2, random_state=57)



clf = XGBClassifier()

clf.fit(X_trainsmoteenn, y_trainsmoteenn)

# DataFrame for result evaluation
#Memisihkan data training dan data testing dengan perbandingan 80:20
X_trainsmoteenn, X_testsmoteenn, y_trainsmoteenn, y_testsmoteenn = train_test_split(X_smoteenn,y_smoteenn,test_size=0.2, random_state=57)
smoteenn_xgb_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])
smoteenn_xgb_result = smoteenn_xgb_result.append(training_xgb(X_trainsmoteenn, X_testsmoteenn, y_trainsmoteenn, y_testsmoteenn, 'TF-IDF 1-grams'), ignore_index = True)

"""# Model Evaluation"""

smoteenn_xgb_result



"""# Word Cloud"""

# Import Library WordCloud. WordCloud digunakan untuk melihat secara visual kata-kata yang paling sering muncul.
# Import Library cv2 untuk mengolah gambar menjadi masking WordCloud

import cv2
from wordcloud import WordCloud

originalImage = cv2.imread('gambar_mask.jpg')
# grayImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2RGB)
(thresh, cloud_mask) = cv2.threshold(originalImage, 100, 255, cv2.THRESH_BINARY)

"""# Wordcloud Sentimen Negatif"""

# WordCloud Label review negatif

tweet_neg = data_ready[data_ready.label_bloob == 'Negatif']
negatif_string = []

for t in tweet_neg.clean_teks:
    negatif_string.append(t)

negatif_string = pd.Series(negatif_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='turbo',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='DeepSkyBlue',
                      max_words=200).generate(negatif_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")

plt.show()

kata_negatif = nltk.word_tokenize(negatif_string)
kata_negatif1 = FreqDist(kata_negatif)
kata_negatif1.most_common(10)

negatif_terbanyak = max(kata_negatif1, key=kata_negatif1.get)
print(negatif_terbanyak)
negatif_muncul = max(kata_negatif1.values())
print(negatif_muncul)

total_negatif = sum(kata_negatif1.values())
total_negatif

persentase_neg = round(negatif_muncul / total_negatif *100,ndigits=2)
persentase_neg

"""# Wordcloud Sentimen Positif"""

# WordCloud sentimen positif

tweet_pos = data_ready[data_ready.Label == 'Positif']
positif_string = []

for t in tweet_pos.clean_teks:
    positif_string.append(t)

positif_string = pd.Series(positif_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='turbo',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='DeepSkyBlue',
                      max_words=100).generate(positif_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")

plt.show()

kata_positif = nltk.word_tokenize(positif_string)
kata_positif1 = FreqDist(kata_positif)
kata_positif1.most_common(10)

positif_terbanyak = max(kata_positif1, key=kata_positif1.get)
print(positif_terbanyak)
positif_muncul = max(kata_positif1.values())
print(positif_muncul)

total_positif = sum(kata_positif1.values())
total_positif

persentase_net = round(positif_muncul / total_positif *100,ndigits=2)
persentase_net





df = pd.read_csv('/content/token_data.csv', sep=';')
df



doc = df['text_preprocessed']
doc

def word_tokenize_wrapper(text):
    return word_tokenize(text)
df['textdata_tokens'] = df['clean_teks'].apply(word_tokenize_wrapper)

df

import gensim
from gensim import corpora

doc = df['textdata_tokens']

dic = corpora.Dictionary(doc)
print(dic)

doc_term_matrix = [dic.doc2bow(doc) for doc in doc]

Lda = gensim.models.ldamodel.LdaModel

total_topics = 3
number_words = 10

lda_model = Lda(doc_term_matrix, num_topics = total_topics, id2word=dictionary, passes=50 )
lda_model.show_topics(num_topics=total_topics, num_words=number_words)



!pip install --upgrade gensim==3.8

import os       #importing os to set environment variable
def install_java():
  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk
  os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"     #set environment variable
  !java -version       #check java version
install_java()

!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
!unzip mallet-2.0.8.zip



import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models.wrappers import LdaMallet
from gensim.models.coherencemodel import CoherenceModel
from gensim import similarities

import os.path
import re
import glob

number_of_topics=30 # adjust this to alter the number of topics
words=20 #adjust this to alter the number of words output for the topic below

os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'
mallet_path = '/content/mallet-2.0.8/bin/mallet' # you should NOT need to change this

number_of_topics=30 # adjust this to alter the number of topics
words=20 #adjust this to alter the number of words output for the topic below
ldamallet30 = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=number_of_topics, id2word=dic, alpha = 0.5)
ldamallet30.show_topics(num_topics=number_of_topics,num_words=words)

