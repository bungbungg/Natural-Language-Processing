# -*- coding: utf-8 -*-
"""sentimen analys 1D CNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T6syEWBvFGwEl1COpnlLheCfGyHs-xWr

# **Stages of modeling :**
https://towardsdev.com/sentiment-analysis-with-cnn-using-keras-c4debff57fc5
"""

from textblob import TextBlob
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D
import numpy as np
import pandas as pd

pip install openpyxl==3.0.5

df = pd.read_excel("/content/indonesia.xlsx", engine='openpyxl')
df.head()

text = df ["Tweet"].tolist()
print(len(text))

y = df["sentimen"]
y = to_categorical(y)
print(y)

df["sentimen"].value_counts()

token = Tokenizer()
token.fit_on_texts(text)
token.index_word

token.word_counts

vocal = len(token.index_word)+1
vocal

x = ['aku lagi galau saudara2']
token.texts_to_sequences(x)

max_kata     = 100
encode_text  = token.texts_to_sequences(text)

X = pad_sequences(encode_text, maxlen = max_kata, padding="post")
X

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 40, test_size= 0.3, stratify=y)

X_train = np.asarray(X_train)
X_test  = np.asarray(X_test)
y_train = np.asarray(y_train)
y_test  = np.asarray(y_test)

vocab  = 300
model  = Sequential()
model.add(Embedding(vocal, vocab, input_length= max_kata))
model.add(Conv1D(64, 8, activation= "relu"))
model.add(MaxPooling1D(2))
model.add(Dropout(0.5))

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(16, activation='relu'))
model.add(GlobalMaxPool1D())
model.add(Dense(3, activation='softmax'))
model.summary()

model.compile(optimizer="adam", loss= "categorical_crossentropy", metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test))

import matplotlib.pyplot as plt
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

def get_encode(x):
    x = token.texts_to_sequences(x)
    x = pad_sequences(x, maxlen = max_kata, padding="post")
    return(x)

x = input("Masukkan kata : ")
x = get_encode(x)

predict = model.predict(x)
classes = np.argmax(predict)
if(classes == 0):
    print("Sentimen      : netral")
if(classes == 1):
    print("Sentimen     : positif")
else:
    print("Sentimen     : negatif")